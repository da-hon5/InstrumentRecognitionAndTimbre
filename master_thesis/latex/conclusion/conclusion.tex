% **************************************************************************************************
% **************************************************************************************************
\newsection{Conclusion}{conclusion:conclusion}
This work introduced a system capable of identifying 15 classes of instruments in music recordings. Furthermore, loudness and timbre of each instrument are estimated. The proposed two-stage system consists of a classifier and a set of so-called timbre estimators. The classifier, a vanilla \gls{cnn} operating on mel-spectrograms, can identify eight instrument families and seven additional specific instruments from a polyphonic mixture. After all instruments are hopefully recognized correctly, the respective timbre estimators predict several timbre descriptors of each instrument which is present in the mix. These timbre estimators exhibit the same network architecture as the classifier, with the only differences being the output activation function and the size of the output layer. As timbre descriptors, primarily spectral features were considered, since those can be computed in a robust and efficient way. All \glspl{cnn} use audio chunks with a duration of four seconds as inputs. At inference, 16 evenly spaced chunks are extracted from each mix and the models' predictions are averaged to obtain the final output.\\

Usually, when applying supervised learning, sufficient training data is needed for each class. However, for many instruments contained in our multi-track datasets, only a few examples are provided. Training a model with so little data inevitably leads to overfitting. In order not to have to discard those classes with few examples, we created a hierarchical taxonomy which is inspired by the Hornbostel-Sachs~\cite{hornbostel1914systematik} system. The first level represents eight instrument families and the second level embodies seven specific instruments. As a result, even instruments with insufficient available data can be utilized for training to help the classifier at identifying their instrument family. The idea is, that the classifier learns a general concept of each family and is therefore able to recognize the families of even the most \enquote{exotic} instruments during inference.\\

For training we used a two-stage approach: We first pre-trained our models with MTG-Jamendo, a music tagging dataset. Afterwards, the models are retrained with three multi-track datasets. We tried different transfer learning methods and came to the conclusion that training the fully connected layers from scratch and fine-tuning the convolutional layers yields the best results.\\

To increase the number of training examples, we produced mixes from the single-instrument tracks of our multi-tracks on-the-fly using two mixing techniques. To create a new mix, tracks are either taken from the same song, yielding musically sounding, time and key-synchronized mixes or tracks are taken from multiple different songs, which obviously leads to totally chaotic mixes. We found out that both strategies have their advantages, hence a combination of musical and non-musical mixes is best.\\

For the classifier, F1-scores above \SI{95}{\percent} were achieved for some classes which is state-of-the-art for instrument recognition in a polyphonic context. The timbre estimators were evaluated using an oracle experiment, i.e. the classifier was assumed to be ideal. It is quite challenging to assess the performance of the timbre estimators, since there is no baseline, each model is tested on a different dataset and the timbre descriptors exhibit distinct order of magnitudes. Nevertheless, we claim to have reached a decent performance.\\



