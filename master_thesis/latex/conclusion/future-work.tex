% **************************************************************************************************
% **************************************************************************************************
\newsection{Future Work}{conclusion:future-work}
Although the outcome of this work is quite satisfying, naturally, there is room for improvement and various topics for potential future research came up.\\

First of all, it would be desirable for the system to support more than 15 classes. Obviously, to achieve that, more data is necessary. However, if datasets keep growing with the same speed as they did in the past years, we might be lucky. Alternatively, few-shot learning could be used to dynamically add new classes, given only a few examples~\cite{garcia2021leveraging}.\\

Secondly, due to time constraints, we did not optimize the architecture of the timbre estimators. Instead, we reused the structure of the classifier and replaced the sigmoid activation function of the last layer. Nonetheless, it would probably make sense to experiment with different architectures for the timbre estimators.\\

Apart from that, the descriptors which are estimated by the timbre estimators should be reassessed in detail for their ability to serve as timbre representations. Because of limited resources, we chose a rather experimental approach to find a suitable set of descriptors. However, a listening test might be of great help. Anyhow, it should be mentioned that the concept of timbre in general is such an extensive topic that it is beyond the scope of this master's thesis.\\

Furthermore, some performance gains could possibly be achieved if both pre-training and transfer learning are improved. With approximately 55,000 tracks, the MTG-Jamendo dataset, which we used for pre-training, is still a few order of magnitudes smaller than the biggest audio datasets available today. Some of those datasets, such as the Million Song Dataset~\cite{bertin2011msd} and the AudioSet~\cite{gemmeke2017audioset}, feature millions of audio recordings and are definitely worth a try. Unfortunately, we did not get access to the audio files of the Million Song Dataset. Besides using larger datasets for pre-training, more sophisticated transfer learning techniques might be able to boost the performance even further. Usually, the last layers of pre-trained \glspl{cnn} are not transferred, since these layers mainly contain information specific to the source task. However, recently a method was proposed~\cite{you2020co}, which allows to transfer the whole network and considerably improve on vanilla transfer learning techniques.\\

As the Slakh dataset was synthesized from MIDI files using software instruments, it is quite dissimilar to real music recordings and some instruments sound totally trashy. Besides that, for many sounds it is hard to assign them to one of the classes of our taxonomy, since most of the instruments sound pretty synthetic and should therefore belong to the synth family. For these reasons, it is doubtful if the dataset is of any use for our system. It would be interesting to evaluate if training with Slakh, in addition to MedleyDB and Mixing Secrets, improves the performance of our models or if the unrealistic data is rather \enquote{confusing} the models.\\

For the sake of simplicity, we randomly split the songs of each multi-track dataset into training, validation and test sets. As a consequence, songs from the same artist or - even worse - from the same album can be present in multiple splits, which in turn might lead to overoptimistic results. Future researchers should keep that in mind and consider using more sophisticated data splits.\\

Since a lot of audio signal processing has to be performed on the CPU while loading the data, training of our models is time-consuming. For this reason, we did not perform extensive hyperparameter tuning but used a simple random search instead. If the efficiency of the data loading procedure could be improved, training times would possibly be significantly shorter. As a consequence, hyperparameters could be optimized in a more sophisticated way and, obviously, finding a better set of hyperparameters might improve the performance of the models.\\

Finally, apart from timbre descriptors, timbre estimators predict the loudness of instruments or instrument families. However, besides loudness, the timbre estimators could be trained to predict other mix parameters, such as the dynamic range or panning information, of the respective instruments as well. Note that for panning estimation, the models have to be provided with stereophonic audio signals, hence the architecture has to be adjusted and the computational cost increases.\\