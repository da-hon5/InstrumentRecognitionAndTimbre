% **************************************************************************************************
% ************************************************************************************************
\newsection{Instrument Recognition (IR)}{fundamentals:instrument_recognition}
\Gls{ir} is a popular problem in \gls{mir}. The goal of early research was to identify single instruments in monophonic recordings. Hereafter, this task will be termed as \textit{solo-instrument recognition}. During the last decades, impressive progress was made in the field of \gls{ir}\footnote{Be aware that results reported in different \gls{mir} papers are often hard to compare, since standardized datasets have been rare in the community. Therefore, researchers regularly used their own private music collections to train and evaluate their models.}. Only approximately ten years ago, people started thinking about the ultimate goal of \gls{ir}; namely \textit{multi-instrument recognition} -- whose aim is to identify all instruments in a polyphonic mixture. The following two sections summarize the development of \gls{ir} from the first attempts in the mid nineties~\cite{kaminsky1995automatic} until the emergence of highly capable \glspl{dnn} around 2015~\cite{li2015automatic}. For an extensive review on the history of this research field, see Chapter 3 of Livshin's PhD thesis~\cite{livshin2007automatic}.

\newsubsection{Solo-Instrument Recognition}{fundamentals:instrument_recognition:solo_instrument}
The first systems could only handle single, isolated tones and research was focused on constructing and evaluating features which enabled good differentiation of instrument classes. These handcrafted features were used to train classical \gls{ml} algorithms such as k-nearest neighbors (k-NN) or suport vector machines (SVM). The earliest paper \cite{kaminsky1995automatic} -- published in 1995 -- deals with the classification of monophonic tones produced by four musical instruments. The authors rely solely on temporal features to train a small feedforward neural network and a k-NN classifier. Although accuracies of up to 100\% were achieved, it is important to mention that the task at hand was very easy and far away from real-world applications, as they considered only single tones in the range of one octave of just four instruments recorded in the same environment. Numerous papers were published over the following years, dealing with the classification of isolated tones. For instance, Eronen and Klapuri \cite{eronen2000musical} computed cepstral coefficients and temporal features like attack time and amplitude modulation frequencies. With these features they trained simple classifiers to predict instrument families and individual instruments from a dataset of single tones of 30 orchestral instruments.\\

After a few years of research on isolated tones, papers addressing \gls{ir} in the context of more realistic instrumental solo recordings emerged. Coping with whole musical phrases instead of isolated tones is far more difficult because notes can overlap or multiple notes can even be played simultaneously in case of polyphonic instruments. Although reported classification accuracies tend to be lower, compared to systems dealing with isolated notes, these systems are way more valuable for real-world applications. Since the estimation of onsets -- which is necessary to compute temporal features -- is prone to failure, most research on \gls{ir} of continuous recordings focuses on spectral and cepstral features. For example, Essid, Richard and David \cite{david2004efficient} showed that the combination of \glspl{mfcc} with other features describing the spectral shape results in high classification accuracy.

\newsubsection{Multi-Instrument Recognition and the Rise of Deep Learning}{fundamentals:instrument_recognition:multi_instrument}
As depicted in Fig.~\ref{fig:multi-instrument-recognition}, multi-instrument recognition strives to identify all instruments in a musical recording. Formally, this is a multi-label classification task since multiple mutually non-exclusive labels can be assigned to each instance.\\
\figc{width=0.7\textwidth}{\pwd/figs/multi-instrument-recognition}{The problem of multi-instrument recognition.}{multi-instrument-recognition}

Although there are some \enquote{conventional} \gls{ml} approaches to multi-instrument recognition, much domain knowledge is necessary and the performance of these systems is mediocre. For instance, \gls{ir} in recordings of jazz quartets have been studied by Essid, Richard and David~\cite{essid2005instrument}. Various combinations of instruments were treated as separate classes and an average accuracy of \SI{53}{\percent} was reported. On the other hand, Heittola, Klapuri and Virtanen~\cite{heittola2009musical} used source separation and multi-pitch estimation as preprocessing steps for their system. They obtained F1-scores of up to \SI{59}{\percent} for mixtures of six signals. In short, all these papers report relatively poor performance which is not surprising, considering that the task at hand is quite challenging.\\

As already mentioned, \gls{ir} was usually approached in two steps using traditional \gls{ml} methods. First, appropriate features are constructed, which requires a lot of domain knowledge. After that, simple \gls{ml} algorithms are trained on these data representations. When, in the 2010s, deep learning (see Section~\ref{sec:fundamentals:deep-learning}) became popular, the \gls{mir} community began to adopt this new technique to their own tasks. One of the first papers using a deep learning approach for \gls{ir} was published in 2015~\cite{li2015automatic}. A \gls{cnn} was trained on raw audio data, achieving a macro F1-score of \SI{64}{\percent} and therefore outperforming traditional methods. Seipel~\cite{seipel2018music} also used a \gls{cnn} for multi-instrument recognition in classical music. However, he utilized mel-spectrograms as input representations and obtained F1-scores of about \SI{80}{\percent}. Music tagging, a task highly related to multi-instrument recognition, was investigated by Won et al.~\cite{won2020evaluation}. The authors evaluated various different \glspl{dnn} on multiple datasets and concluded that a simple VGG-ish~\cite{simonyan2014very} architecture obtained a ROC-AUC of up to 0.91, outperforming almost all other models. We would like to emphasize at this point, that comparing \gls{ir} systems of different authors is tricky, since until now there is no uniform benchmark dataset available. For this reason, almost every researcher uses a different dataset.