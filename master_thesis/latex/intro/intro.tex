% **************************************************************************************************
% **************************************************************************************************
The gigantic proportions of present music databases entail unprecedented opportunities as well as new challenges for the music industry. Even a laptop's hard disk can store tens of thousands of hours of music, which corresponds to several terabytes. In order to tame such vast amounts of audio data, sophisticated algorithms are developed by the \gls{mir} community. Since deep learning, a subarea of \gls{ml} which relies on extremely large datasets for training and evaluation, became the primary approach for tackling most problems in \gls{mir}, research benefits greatly from today's abundance of data. Systems for managing music databases try to automatically extract information such as genre, instrumentation, key, etc. from the audio and assign labels to each instance in this respect. The aforementioned task is referred to as \textit{classification} if the labels are discrete classes. The obtained labels make it possible to effectively browse huge databases. For example, a user could retrieve all recordings containing specific instruments. Besides discrete labels, deep learning models can also be trained to predict continuous variables -- a task called \textit{regression}. For instance, various audio features of a recording could be estimated by neural networks. In this thesis, we present a system which comprises models both for classification and regression.\\

Humans are incredibly gifted when it comes to analyzing music or interpreting complex acoustic scenes. Consider the famous \textit{cocktail party effect}, the ability of the brain to filter out unwanted noise and solely focus on a single stimulus, such as a conversation. In order to teach computers how to listen, research is therefore often inspired by the human ear-brain system. For instance, artificial neural networks are inspired by the brain and mel-spectrograms, often used as input representations of audio signals in deep learning, mimic the way we perceive pitch. Although a lot of research in the field of \gls{mir} is conducted, musically trained individuals are still superior to computer algorithms when it comes to most music classification and regression tasks. However, with the advent of deep learning, the gap between humans and machines decreased rapidly.