% **************************************************************************************************
% **************************************************************************************************
\newsection{Scope of the Thesis}{intro:scope}
Imagine you are a music producer and you fell in love with the timbre of a particular instrument in a recording, let's assume that the thin piano sound from the song \textit{Lady Madonna} by \textit{The Beatles} appeals to you. As your sample library contains thousands of piano loops, you are sure that there must be a similar-sounding one among them. It just needs to be found. After hours of manually browsing your sample packs, listening to every recording, you give up and question your decision to become a music producer. You ask yourself if this time-consuming and boring task can somehow be automated by a computer algorithm. But what are the qualities of a suitable system and how can it be implemented? These are some of the questions we try to answer in this thesis\footnote{Note that this fictional example only demonstrates a specific use case of such a system. However, other applications are presented in this thesis as well.}.\\

A system, enabling the user to browse a database by instrumentation and timbre, has to solve two separate problems from the field of \gls{mir}. Initially, all instruments in a music recording have to be identified. This task, known as \textit{multi-instrument recognition}, is a popular area of research in \gls{mir}. After the instruments have hopefully been classified correctly, some timbre-specific information for each instrument contained in the mixture has to be extracted. In Chapter~\ref{chp:method}, the proposed two-stage system will be discussed in detail.\\

As already mentioned, the music recording to be analyzed is first examined by a classifier, in order to identify every instrument contained in the mix. For the classifier we adopt an architecture proposed in a work on music tagging~\cite{won2020evaluation} -- a task highly related to multi-instrument recognition. The network is a standard \glsfirst{cnn} with seven convolutional layers followed by two fully connected layers. Since models like that have originally been developed for image processing, 2D audio representations are expected as inputs. For this reason, we use so-called \textit{mel-spectrograms}: a variant of vanilla spectrograms where frequency bins are mapped to the mel scale; a scale of pitches discovered by psychoacoustic listening tests. The \gls{cnn} attempts to find patterns in these time-frequency representations -- a challenging task, because several instruments might be active simultaneously, resulting in overlapping partials. This is especially true if all involved instruments play in the same key and tempo, which is usually the case in conventional western music. Since we have to use multi-track data to train the timbre estimation models anyway, we can exploit the individual tracks for data augmentation as well. Therefore, mixes are randomly created on-the-fly while training the neural networks using two different mixing strategies. As three distinct multi-track datasets are utilized in this work, each dataset exhibiting its own class structure, a unified taxonomy has to be found. Furthermore, there is a lack of data for many instruments, which is a problem when working with \glspl{dnn}. In order to still exploit underrepresented classes, we propose a two-level hierarchical taxonomy. To further boost our models' performance, we initially pre-train them on a large music tagging dataset before fine-tuning with the multi-tracks. For this reason, an experiment comparing different transfer learning approaches is conducted.\\

The second part of our system comprises a set of \glspl{dnn} to predict timbre descriptors for various target instruments in the recording. Hereafter, these models will be denoted as \textit{timbre estimators}. Several audio features are tested for their ability to serve as timbre representations. After some investigation, we opt for the classical \gls{mfcc} in combination with spectral centroid, zero-crossing rate and spectral flatness. \Glspl{mfcc}, which basically describe the envelope of the magnitude spectrum, have proven useful as timbre descriptors for several decades. In addition to those timbre descriptors, we try to estimate the loudness of each instrument. The task of timbre estimation is related to source separation but instead of estimating a target instrument's audio signal, we strive to predict timbre descriptors of a specified instrument from a mix. Inspired by source separation systems, one model per instrument class is trained to predict a feature vector representing the timbre of all instruments from this class which are contained in the recording. The resulting feature space can be harnessed, for example, to determine the similarity of different instruments regarding timbre, by computing distances between their embeddings. A music producer might use this space as a creative tool to explore timbre possibilities or to analyze mixes of other artists.\\

Let's bring the fictional example from the beginning of this section into play again. You, who are interested in the sound of Paul McCartney's piano in \textit{Lady Madonna}, can now compare the piano's timbre representation from the original song with your own sample library to retrieve similar recordings in terms of timbre. Besides that, our proposed system could be deployed to automatically label large music datasets, which are crucial for training and evaluation of \gls{ml} methods, especially deep learning algorithms.


