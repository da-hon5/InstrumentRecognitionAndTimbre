% **************************************************************************************************
% **************************************************************************************************
\newsection{Pre-Processing and Data Augmentation}{method:preprocessing}
Overfitting is a common problem in deep learning when too little data is available for a given task. Unfortunately, datasets for \gls{mir} tend to be relatively small. Therefore, it is usually necessary to artificially increase the number of instances by creating new samples out of existing ones. This process, known as \textit{data augmentation}, is quite challenging in the course of this work. Since the quantity of data samples, which can be generated by random processing and mixing of individual sources, is infinite, and therefore impossible to store on a harddrive, data augmentation has to be done on-the-fly. Recently, python libraries for fast parallel augmentation of whole batches on the GPU emerged. However, the numerous processing steps during data loading still have to be computed on the CPU when working with multi-tracks. This turned out to be a massive bottleneck during training of our models. In order to reduce computation time, the audio data is converted to mono and downsampled to \SI{32}{\kilo\hertz}\footnote{Note: Most researchers dealing with music classification reduce the sample rate even further (\SI{16}{\kilo\hertz} is quite common) and still report decent performance. However, for timbre estimation, we assume that a higher audio quality is favorable.} in a pre-processing step. Using a lower sample rate is acceptable, as there is only little information relevant to the human ear above \SI{16}{\kilo\hertz}.\\

In this work, the following data augmentation techniques are applied on-the-fly while training the \glspl{dnn}:
\begin{itemize}
	\item random selection of a four-second chunk in a song
	\item random mixing of individual sources
	\item random gain
	\item random filters (high shelf, low shelf and peaking filter)
	\item random time stretching
	\item random pitch shifting
\end{itemize}
Our main data augmentation approach is to create new unique mixtures out of the single-instrument tracks (sources) from the multi-tracks. Therefore, we propose two mixing strategies -- \textit{musical mixing} and \textit{non-musical mixing}. Musical mixing means that only sources from the same song are combined to generate a new mix, whereas non-musical mixing signifies that each source in a mix originates from a different song. The latter, which was successfully used in music source separation~\cite{uhlich2017improving}, obviously results in non-musical, strange sounding mixes, as each instrument plays in a different key and tempo. Nevertheless, the number of training examples can be increased massively by this mixing strategy, which in turn improves generalization. Musical mixing, on the other hand, naturally produces well-sounding results, since all sources originate from the same song and are therefore uniform in tempo and key. In order to investigate the performance of the two proposed mixing strategies, an experiment is conducted in Chapter~\ref{chp:results}. Note that for validation and testing, musical mixing was used exclusively, because in real-world recordings, instruments will almost always play in the same key and tempo. Also note that the non-musical mixing strategy could be extended with pitch and tempo synchronization to obtain mixes similar to the ones produced by musical mixing~\cite{kratimenos2021augmentation}. However, due to the substantial computational cost of synchronizing pitch and tempo, which require F0 estimation and BPM detection, we decided not to implement these algorithms. Bear in mind that the data loading procedure used in this work already entails a high CPU utilization, leading to a serious bottleneck during training. Therefore, we wanted to avoid any additional workload.\\

In Fig.~\ref{fig:data-loading}, the complete data loading procedure is depicted, illustrating how a polyphonic mixture is constructed from individual sources. Be aware that each block in Fig.~\ref{fig:data-loading} uses randomness. Hyperparameters related to data loading are displayed in bold. In Section~\ref{sec:results:multi-inst_recognition:hyperparameters}, a random search is conducted to tune most of these hyperparameters. The obtained values can be found in Table~\ref{tab:other-hyperparam}. In the following paragraph, the data loading procedure is explained in more detail. For the implementation of the data augmentation pipeline -- random gain, filters, time stretching and pitch shifting -- the open source library \textit{audiomentations}~\cite{audiomentations} was used. The settings of these audio effects were optimized by a random search and reported in Table~\ref{tab:augmentation-hyperparam}.\\

Initially, one of the three multi-track datasets is selected. Therefore, we set $p_{medleyDB}$, $p_{mixingsecrets}$ and $p_{slakh}$ to $1/3$, i.e. each dataset is selected with the same probability. This step is necessary because the Slakh dataset is much larger than the other two multi-track datasets. Simply merging them and sampling from the combined dataset would lead to a domination of Slakh samples. However, this should be avoided, because, as already mentioned, the Slakh dataset is synthesized from MIDI files and is therefore quite unrelated to real-world music recordings. In the next step, one of the two proposed mixing strategies is selected. In other words, it is determined whether all sources are sampled from the same song or if each source can originate from a different song. Therefore, musical mixing is applied with probability $p_{musical}$ and non-musical mixing is used with probability $1 - p_{musical}$. As our models should be able to cope with solo-instrument recordings as well, we load single sources (number of sources equals one) with a certain probability $p_{single-source}$. After choosing the number of sources $N$ in the mix\footnote{$N$ is sampled from a uniform distribution $N \sim \mathcal{U}\{1, \, Max \, \#sources\}$. In case of musical mixing, $Max \, \#sources$ is determined by the number of single-instrument tracks, which exist for a given song.}, the following steps are performed $N$ times for the non-musical mixing method: select a song, select a source from this song\footnote{Since percussion and plucked strings instruments are overrepresented in the data, we want to skip sources belonging to these two families from time to time. For this reason, the probabilities $p_{skip-percussion}$ and $p_{skip-plucked-str}$ are introduced.}, select a four-second chunk from this source, apply some digital effects on the audio and add it to the mix. Four audio effects, which have been tried and tested for data augmentation in \gls{mir}, are utilized (each with a certain probability): amplitude scaling (gain), three types of filters (high shelf, low shelf and peaking filter), pitch shifting and time stretching. The order of these effects as well as all their parameters are random. Note that, in the musical mixing case, all sources originate from the same song, hence a song is selected only once and not at every loop iteration. Furthermore, the time position of all chunks has to be identical in order to obtain musical sounding results. Finally, pitch shifting and time stretching are omitted for the same reason when musical mixing is applied.
\figc{width=0.8\textwidth}{\pwd/figs/data-loading}{Data loading process: Thick arrows represent audio signals. Hyperparameters are displayed in bold.}{data-loading}


