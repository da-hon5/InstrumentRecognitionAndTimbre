% **************************************************************************************************
% **************************************************************************************************
\newsection{Two-Stage System}{method:system}
Although it is theoretically possible to combine multi-instrument recognition and timbre estimation in a single multi-task model with shared parameters, such an approach can cause various problems. Since parameters are shared between several different tasks, a phenomenon known as \textit{negative transfer} can occur. This means that the individual tasks have a negative influence on each other, in other words, improving the performance on one task decreases the performance on another one~\cite{crawshaw2020multi}. Negative transfer is common if tasks are unrelated or the shared layers are too small. Apart from that, multi-task models tend to be big and complex. Therefore, debugging and fine-tuning can be challenging. For these reasons, we decided to split the two tasks and construct a two-stage system instead.\\

The proposed system, as depicted in Fig.~\ref{fig:two-stage_system}, consists of two parts -- a \textit{classifier} and a set of \textit{timbre estimators}. Both subsystems are fed with four-second chunks of the audio mixture. For more info on how this mixture is created and the preprocessing steps involved, see Section~\ref{sec:method:preprocessing}. For each chunk in a batch, the classifier detects present instruments and instrument families according to the taxonomy presented in Section~\ref{sec:method:taxonomy}. Subsequently, a set of timbre estimators -- one for each class -- predicts various timbre descriptors and the loudness of all instruments or instrument families in the mix. Bear in mind that every timbre estimator is trained to predict features of only one target instrument or instrument family respectively, hence the need for several of these networks. This approach is inspired by source separation systems, where it is a common practice to employ a separate model for each target. In the example shown in Fig.~\ref{fig:two-stage_system}, the mix to be analyzed contains maracas, a percussion instrument, and an electric guitar, which belongs to the family of plucked string instruments. After the classifier hopefully identifies the percussion and plucked string families as well as the electric guitar class, the corresponding timbre estimators predict both loudness and timbre descriptors of each instrument or instrument family respectively. Note that, in this example, the electric guitar is the only instrument of the plucked string family. Therefore, timbre estimators for electric guitar and plucked string should ideally yield the same output.\\

As mentioned earlier, both classifier and timbre estimators operate on four-second audio chunks. During training, a batch consists of 16 chunks from different mixes. During inference, batches are composed of 16 evenly spaced chunks from the same mix and predictions over all these chunks are averaged to obtain a final output. For more details regarding data loading see Section~\ref{sec:method:preprocessing}. In the next sections, the architecture of the involved networks will be examined in detail.
\figc{width=0.75\textwidth}{\pwd/figs/two-stage_system}{Proposed two-stage system. First, the classifier identifies instruments and families in the recording. Subsequently, the corresponding timbre estimators predict timbre descriptors and loudness of all instruments and families involved in the recording.}{two-stage_system}

\newsubsection{Classifier}{method:system:multi-instr-rec}
The classifier's architecture was adopted from~\cite{won2020evaluation}. In this publication, different state-of-the-art music tagging models are evaluated. The authors of the paper concluded that a simple \gls{cnn} trained on short audio chunks outperformed almost all competitors. The proposed network is very similar to a VGG-Net~\cite{simonyan2014very} commonly used for image classification. In Fig.~\ref{fig:classifier}, the architecture of this \gls{cnn} is illustrated. A stack of convolutional layers, hereafter referred to as \textit{backbone}, is followed by fully connected layers. Before we continue with the classifier, we should briefly talk about the backbone at this point. The architecture of the backbone is depicted in Fig.~\ref{fig:backbone}. In an initial step, the time domain signal is converted to a mel-spectrogram with 128 frequency bins using \textit{librosa}~\cite{mcfee2015librosa}. A window length of 512 and a hop size of 256 are used to compute the spectrogram. After that, seven convolutional blocks are employed to find relevant patterns in this time-frequency representation. Each block consists of a 2D convolutional layer with a kernel size of $3\times3$ followed by batch normalization, ReLU activation and $2\times2$ max pooling. After seven max pooling layers, the frequency dimension is reduced to one. Finally, 1D max pooling is applied over the time axis to shrink the time dimension to one as well. Since the last convolutional layer has 512 kernels, the resulting output representation of the backbone also has a size of 512. After the backbone extracted suitable features from the mel-spectrograms, subsequent fully connected layers, also referred to as dense layers, learn non-linear combinations of theses features and finally make a decision whether a certain class is active or not. Between the fully connected layers, batchnorm and dropout are applied -- see Section~\ref{sec:fundamentals:deep-learning:cnn} for an introduction on their functionality and benefits. In the last layer of the classifier, a sigmoid activation function maps the model's predictions to values between zero and one, hence the outputs can be interpreted as probabilities.
\figc{width=0.36\textwidth}{\pwd/figs/classifier_architecture}{Architecture of the classifier.}{classifier}
\figc{width=0.6\textwidth}{\pwd/figs/backbone_architecture}{Architecture of the backbone.}{backbone}

\newsubsection{Timbre Estimators}{method:system:feature-pred}
As mentioned earlier, timbre estimators extract various timbre descriptors of a target instrument or family from a polyphonic mixture. The task of simultaneously predicting several real-valued variables is called \textit{multi-output regression}~\cite{borchani2015survey}. The only difference to multi-label classification is that the target variables are continuous instead of binary. For this reason, we can reuse the architecture of the classifier from Fig.~\ref{fig:classifier} for the timbre estimators. Besides adjusting the size of the output layer, we only have to replace the sigmoid activation function of the last layer with a linear activation function, i.e. no activation function. In Fig.~\ref{fig:feat-pred}, the architecture of the timbre estimators is depicted.\\
\figc{width=0.4\textwidth}{\pwd/figs/feature-predictors_architecture}{Architecture of the timbre estimators.}{feat-pred}

A particularly challenging task is the selection of descriptors, which should be estimated by the timbre estimators and subsequently serve as a timbre representation of the respective instrument class or family. Since the perception of timbre is very subjective, a listening test would be a good way to find suitable descriptors. However, we did not have the resources to realize a time-consuming listening test. Therefore, a simple experiment was conducted, in order to identify a good set of descriptors, with the ability to cluster similar-sounding examples. First of all, we extracted a two-second chunk from every single-instrument track of our multi-track datasets. We then computed several different timbre descriptors, presented in Section~\ref{sec:fundamentals:timbre}, and concatenated them. In other words, we obtained multidimensional embeddings of our examples in a timbre space. To visualize these embeddings in 2D, a t-SNE~\cite{van2008tSNE} algorithm was applied and the resulting x- and y-coordinate of each sample was passed to a simple C++ application\footnote{https://github.com/da-hon5/AudioScatterPlot} (a slightly modified version of~\footnote{https://ml4a.github.io/guides/AudioTSNEViewer}). The GUI of the app is depicted in Fig.~\ref{fig:tsne-app}. This application creates an interactive scatter plot where the corresponding audio file is played back when the mouse is moved over a datapoint. Therefore, the embedding space can be explored by simply hovering over the map and inspecting the data by ear. In case that suitable timbre descriptors have been found, distances between data samples represent the similarity of sounds, i.e. similar-sounding examples should be close to each other. Since, in the past, good \gls{ir} results have been reported using solely \glspl{mfcc}~\cite{david2004efficient}, we considered 12 \glspl{mfcc} as a starting point for our experiments. We then added other timbre descriptors and inspected if the additional information leads to better timbre representations. After many iterations of this procedure, the final set of descriptors consists of 12 \glspl{mfcc}, spectral centroid (SPC), spectral flatness (SPF) and zero-crossing rate (ZCR). As already mentioned in Section~\ref{sec:fundamentals:timbre}, onset detection and F0 estimation are prone to errors when dealing with audio material other than isolated notes. Considering that these two algorithms are inevitable to compute most temporal and harmonic features, we focused on the more stable and easy to compute spectral descriptors.\\ 

Note that, in order to obtain the targets for training, silent parts of the audio signals are discarded before computing the timbre descriptors. This procedure is necessary, as we want to characterize the timbre of an instrument when it is actually playing. Computing timbre descriptors of silent parts would result in distorted targets. Therefore, we divide each four-second chunk into eight pieces, calculate the respective \gls{rms} and dump all sections below a threshold of \SI{-70}{\decibel}. All timbre descriptors are computed using \textit{librosa}~\cite{mcfee2015librosa} with a frame size of \SI{64}{\milli\second} -- 2048 samples at a sample rate of \SI{32}{\kilo\hertz} -- and \SI{25}{\percent} overlap. Median and interquartile range are calculated for each descriptor to aggregate all frames into a single dimension, hence we end up with a 30-dimensional timbre representation (2*12 \glspl{mfcc} + 2 SPC + 2 SPF + 2 ZCR). Since we want our models to predict the loudness as well, the timbre estimators' final output layer comprises 31 neurons. The loudness targets for training are computed using the algorithm described in Fig.~\ref{fig:itu-loudness-algorithm}.\\
\figc{width=0.8\textwidth}{\pwd/figs/tsne-app}{GUI of the t-SNE app. As an example, embeddings of the instruments from the plucked strings family (green=electric guitar, blue=acoustic guitar, red=electric bass, orange=other) are plotted.}{tsne-app}

Inspired by prototypical networks, we also investigated if it is possible to perform instrument classification based only on the distance between the embedding of an example in our timbre space and a class prototype. We observed that the embeddings' variances within the classes are too high, in other words, instruments from the same class can sound very differently. As a consequence, it is impossible to build a reliable classifier solely on timbre descriptors. Although such feature engineering approaches for \gls{ir} worked well in the past (see Section~\ref{sec:fundamentals:instrument_recognition:solo_instrument}), researchers addressed much simpler cases of \gls{ir} -- often isolated monophonic tones recorded in the same environment were considered. Thus, the variances within the classes tended to be very small compared to the music recordings we used for our experiments.

